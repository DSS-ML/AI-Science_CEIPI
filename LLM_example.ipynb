{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8be82ee-3923-4a39-a950-995c15751050",
   "metadata": {},
   "source": [
    "# PART 1: HUGGING FACE TRANSFORMERS\n",
    "### Goal: Use a pre-trained model for summarization with minimal code.\n",
    "### Connection to Slide 10: The \"democratization\" of AI access via Model Hubs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed720349-5a4d-420e-8243-4c231a8b4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading model from Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dilet\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dilet\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      ">>> Generating Summary...\n",
      "Original Length: 435 characters\n",
      "Summary:  The Transformer architecture abandoned sequential processing, introducing parallel processing of all tokens simultaneously . For each token, the model computes an  \"attention score\" with every other token, creating a rich map of contextual\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Initialize the pipeline.\n",
    "# The 'pipeline' abstraction handles model downloading, tokenization, and inference automatically.\n",
    "# We use a small, efficient model (distilbart) suitable for CPU usage.\n",
    "print(\">>> Loading model from Hugging Face Hub...\")\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "# 2. Input Data: A dense technical paragraph (Simulating a paper abstract).\n",
    "# This text describes the Transformer architecture (Slide 7 content).\n",
    "scientific_text = \"\"\"\n",
    "The Transformer architecture abandoned sequential processing, introducing parallel \n",
    "processing of all tokens simultaneously. For each token, the model computes an \n",
    "\"attention score\" with every other token, creating a rich map of contextual relationships. \n",
    "This allows the model to learn which relationships matter, regardless of distance. \n",
    "Traditional RNNs suffered from vanishing gradients and could not be parallelized efficiently.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Inference: Generate the summary.\n",
    "# Parameters:\n",
    "# - max_length: constrains the output size.\n",
    "# - do_sample=False: makes the output deterministic (Temperature = 0).\n",
    "print(\"\\n>>> Generating Summary...\")\n",
    "summary = summarizer(scientific_text, max_length=45, min_length=15, do_sample=False)\n",
    "\n",
    "# 4. Output the result.\n",
    "print(f\"Original Length: {len(scientific_text)} characters\")\n",
    "print(f\"Summary: {summary[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410c9d7-c5de-4c7d-864d-f242ba7a3b3a",
   "metadata": {},
   "source": [
    "# PART 2: OLLAMA (LOCAL INFERENCE)\n",
    "### Goal: Run a quantized LLM (Llama 3) locally on your laptop.\n",
    "### Connection to Slide 28: Implementing the \"Expert Persona\" via System Prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5f6228-a814-4792-8830-204721367f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Sending request to local model (User Input: We introduce a novel instrumental variable approach to estimate the elasticity of labor supply.)...\n",
      "\n",
      ">>> LOCAL OLLAMA RESPONSE:\n",
      "**METHODOLOGICAL** because the abstract focuses on a new methodological technique (instrumental variable approach) for estimating labor supply elasticity.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 1. Define the System Prompt.\n",
    "# We instruct the model to act as a specialized domain expert (JEL Classifier).\n",
    "# See Slide 28 regarding the \"JEL Expert Classification Task\".\n",
    "system_instruction = \"\"\"\n",
    "You are a strict JEL (Journal of Economic Literature) classification expert.\n",
    "Analyze the user's abstract and classify it as either 'METHODOLOGICAL' or 'TOPIC' based.\n",
    "Provide a one-sentence justification.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Define the User Input (A simulated abstract).\n",
    "user_abstract = \"We introduce a novel instrumental variable approach to estimate the elasticity of labor supply.\"\n",
    "\n",
    "print(f\">>> Sending request to local model (User Input: {user_abstract})...\")\n",
    "\n",
    "# 3. Call the local API with CONTROL PARAMETERS.\n",
    "# This runs entirely on your machine's hardware (CPU/GPU).\n",
    "response = ollama.chat(\n",
    "    model='mannix/gemma2-9b-simpo:latest',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'system',\n",
    "        'content': system_instruction,\n",
    "      },\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': user_abstract,\n",
    "      },\n",
    "    ],\n",
    "    # PARAMETERS CONFIGURATION\n",
    "    options={\n",
    "        # Temperature: Controls the \"creativity\" or randomness of the model.\n",
    "        # 0.0 = Deterministic/Precise. The model always picks the most likely word. Best for classification.\n",
    "        # 1.0 = Creative. The model takes more risks. Best for writing stories.\n",
    "        'temperature': 0.1,\n",
    "\n",
    "        # Top K: Limits the model's choice to the top K most probable next words.\n",
    "        # e.g., 40 means \"Only consider the 40 most likely words, ignore the thousands of others.\"\n",
    "        # Helps prevent the model from going off-topic with nonsense words.\n",
    "        'top_k': 40,\n",
    "\n",
    "        # Top P (Nucleus Sampling): A dynamic threshold.\n",
    "        # The model selects from the smallest set of words whose cumulative probability equals P.\n",
    "        # 0.8 is standard; lowering it makes the model more conservative/focused.\n",
    "        'top_p': 0.8,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4. Print the local response.\n",
    "print(\"\\n>>> LOCAL OLLAMA RESPONSE:\")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d06b87-9078-4b53-9da0-1bee780a0168",
   "metadata": {},
   "source": [
    "# PART 3: CEREBRAS (CLOUD ACCELERATION & CONTROL)\n",
    "### Goal: Access a Frontier Model (70B) to experiment with \"Temperature\" and \"Creativity\".\n",
    "### Connection to Slide 6: Solving the \"Parallelization Constraint\" with specialized hardware.\n",
    "### Connection to Slide 5: Controlling the \"Cost of Prediction\" (Accuracy vs. Hallucination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6283fce-9e66-47ec-83d3-cb2be7639677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> CEREBRAS (Conservative Mode - Temp 0)\n",
      "\n",
      ">>> Query: Explain the concept of 'Self-Attention' in Transformers using the analogy of a library filing system.\n",
      "\n",
      "**Introduction to Self-Attention**\n",
      "=====================================\n",
      "\n",
      "The concept of self-attention in Transformers can be understood using the analogy of a library filing system. Imagine a vast library with an infinite number of books, each representing a piece of information. The library's filing system is designed to help you find relevant information quickly.\n",
      "\n",
      "**The Library Filing System Analogy**\n",
      "--------------------------------------\n",
      "\n",
      "In this analogy:\n",
      "\n",
      "*   **Books** represent the input sequence (e.g., a sentence or a document) that is being processed.\n",
      "*   **Book Titles** represent the individual elements (e.g., words or tokens) within the input sequence.\n",
      "*   **Book Shelves** represent the different positions or contexts in which the elements appear.\n",
      "*   **Librarian** represents the self-attention mechanism.\n",
      "\n",
      "**How Self-Attention Works**\n",
      "-----------------------------\n",
      "\n",
      "Here's how the self-attention mechanism works in the context of the library filing system:\n",
      "\n",
      "1.  **Querying the Librarian**: When you ask the librarian to find information related to a specific topic, the librarian (self-attention mechanism) creates a **query** based on the topic.\n",
      "2.  **Scanning the Book Shelves**: The librarian scans the book shelves (input sequence) to find books (elements) that are relevant to the query.\n",
      "3.  **Calculating Relevance**: For each book, the librarian calculates a **relevance score** based on how well the book's title (element) matches the query. This is done using a **similarity function** (e.g., dot product).\n",
      "4.  **Weighting the Books**: The librarian assigns a **weight** to each book based on its relevance score. The weights represent the importance of each book in the context of the query.\n",
      "5.  **Retrieving Relevant Information**: The librarian uses the weighted books to retrieve the relevant information. This is done by taking a **weighted sum** of the book contents (element values).\n",
      "\n",
      "**Multi-Head Attention**\n",
      "-------------------------\n",
      "\n",
      "In the Transformer architecture, self-attention is applied multiple times in parallel, with different weight matrices. This is known as **multi-head attention**. In the library analogy, this would be like having multiple librarians, each with their own set of book shelves and querying mechanisms. Each librarian would retrieve relevant information independently, and the results would be combined to produce the final output.\n",
      "\n",
      "**Code Example**\n",
      "----------------\n",
      "\n",
      "Here's a simplified example of self-attention in PyTorch:\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class SelfAttention(nn.Module):\n",
      "    def __init__(self, embed_dim, num_heads):\n",
      "        super(SelfAttention, self).__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        self.query_linear = nn.Linear(embed_dim, embed_dim)\n",
      "        self.key_linear = nn.Linear(embed_dim, embed_dim)\n",
      "        self.value_linear = nn.Linear(embed_dim, embed_dim)\n",
      "        self.dropout = nn.Dropout(0.1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # Split the input into query, key, and value\n",
      "        query = self.query_linear(x)\n",
      "        key = self.key_linear(x)\n",
      "        value = self.value_linear(x)\n",
      "\n",
      "        # Calculate the attention scores\n",
      "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.embed_dim)\n",
      "\n",
      "        # Apply softmax to get the attention weights\n",
      "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
      "\n",
      "        # Apply dropout to the attention weights\n",
      "        attention_weights = self.dropout(attention_weights)\n",
      "\n",
      "        # Calculate the output\n",
      "        output = torch.matmul(attention_weights, value)\n",
      "\n",
      "        return output\n",
      "\n",
      "# Example usage\n",
      "embed_dim = 128\n",
      "num_heads = 8\n",
      "batch_size = 32\n",
      "sequence_length = 100\n",
      "\n",
      "x = torch.randn(batch_size, sequence_length, embed_dim)\n",
      "\n",
      "self_attention = SelfAttention(embed_dim, num_heads)\n",
      "output = self_attention(x)\n",
      "```\n",
      "Note that this is a highly simplified example and actual implementations may vary depending on the specific use case and requirements.\n",
      "\n",
      "**Conclusion**\n",
      "==============\n",
      "\n",
      "In conclusion, the concept of self-attention in Transformers can be understood using the analogy of a library filing system. The self-attention mechanism acts like a librarian, querying the input sequence to find relevant information and weighting the importance of each element based on its relevance to the query. This allows the model to focus on the most important elements in the input sequence and capture long-range dependencies."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = Cerebras(api_key=os.environ.get(\"CEREBRAS_API_KEY\"))\n",
    "\n",
    "print(\">>> CEREBRAS (Conservative Mode - Temp 0)\")\n",
    "\n",
    "complex_query = \"Explain the concept of 'Self-Attention' in Transformers using the analogy of a library filing system.\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": complex_query}\n",
    "    ],\n",
    "    stream=True,\n",
    "    # CONFIGURATION: PRECISE\n",
    "    temperature=0.0,  # Zero randomness. Picks the single most likely next token\n",
    "    top_p=0.9         # Consider all possibilities, but strictly ranked\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> Query: {complex_query}\\n\")\n",
    "for chunk in stream:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08bdd97-f3ff-4575-87af-4d890cda7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> CEREBRAS (Creative Mode - Temp 1.5)\n",
      "\n",
      ">>> Query: Explain 'Self-Attention' like a fantasy storyteller using a library analogy. Be colorful and metaphorical.\n",
      "\n",
      "Gather 'round, travelers of the realm, and heed my tale of the mystical library of information, where ancient tomes hold the secrets of the universe. Within this hallowed hall, a peculiar magic dwells, known as \"Self-Attention.\" 'Tis a wondrous force, akin to a curious scribe, tasked with unraveling the very essence of the knowledge contained within.\n",
      "\n",
      "In this enchanted library, shelves upon shelves of dusty leather-bound books stretch far and wide, each one a container of wisdom, waiting to impart its contents to the inquisitive mind. Imagine, if you will, a noble tome, adorned with golden filigree and strange symbols, representing a sentence, a thought, or an idea. This tome, like its companions, holds various passages, each a distinct word or concept, nestled within its pages.\n",
      "\n",
      "As our intrepid scribe, Self-Attention, begins its quest, it casts a discerning gaze upon the tome, illuminating specific passages that seem to whisper secrets to one another. The words \"dragon,\" \"hoard,\" and \"cave\" might catch its attention, for they appear to conspire, weaving a narrative of ancient riches hidden within a mysterious cavern. As Self-Attention focuses on these phrases, it weights their importance, acknowledging that the word \"cave\" is perhaps more crucial than \"rocky\" or \"path\" in understanding the tale.\n",
      "\n",
      "With this realization, the scribe conjures an ethereal landscape, where interconnected pathways reveal the relationships between words. It invites the terms \"dragon\" and \"hoard\" to converse, sharing their context and meanings, and fostering a profound understanding between them. Self-Attention becomes the arbiter of these conversations, ensuring that the interactions between \"cave,\" \"treasure,\" and \"ancient lore\" are distilled into an essence, or a weighted representation, of the information.\n",
      "\n",
      "Now, as the scribe continues to roam the halls of knowledge, its quest expanding to encompass multiple tomes and narratives, the tapestry of understanding grows richer and more intricate. The mystic bonds between terms within and across sentences unfold, revealing hidden truths and patterns. In this self-referential realm, the boundaries between books and pages dissolve, yielding a majestic vista of interconnected thoughts, concepts, and stories.\n",
      "\n",
      "Thus, within the hallowed library of information, Self-Attention, our loyal scribe, unravels the mysteries of the tomes, crafting an odyssey of knowledge that invites exploration and discovery. This enchanting mechanism, by acknowledging the intricate relationships within and between ideas, distills the essence of language and cognition, allowing us to navigate the grand repository of wisdom with wonder and awe.\n",
      "\n",
      "Now, brave travelers, as you venture forth into the vast expanse of the knowledge realm, may the magical forces of Self-Attention guide and enlighten your path, granting thee the hidden knowledge and ancient wisdom contained within the leather-bound pages of the mystical tomes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = Cerebras(api_key=os.environ.get(\"CEREBRAS_API_KEY\"))\n",
    "\n",
    "print(\">>> CEREBRAS (Creative Mode - Temp 1.5)\")\n",
    "\n",
    "# Nota: Per vedere l'effetto creativo, chiediamo uno stile narrativo\n",
    "complex_query = \"Explain 'Self-Attention' like a fantasy storyteller using a library analogy. Be colorful and metaphorical.\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": complex_query}\n",
    "    ],\n",
    "    stream=True,\n",
    "    # CONFIGURATION: CREATIVE\n",
    "    temperature=1.5,  # High randomness. The model takes risks with rare words.\n",
    "    top_p=1       # craziest words probably nonsense.\n",
    ")\n",
    "\n",
    "print(f\"\\n>>> Query: {complex_query}\\n\")\n",
    "for chunk in stream:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(Docling)",
   "language": "python",
   "name": "docling-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
